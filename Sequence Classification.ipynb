{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle as pkl\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from disk\n",
    "\n",
    "df = pd.read_csv('~/Data/IMDB_Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y'] = df['sentiment'].apply(lambda x : 0 if x == 'negative' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find stop words\n",
    "nlp = spacy.load('en')\n",
    "stops = nlp.Defaults.stop_words\n",
    "retain_words = ['always', 'nobody', 'cannot', 'none', 'never', 'no', 'not']\n",
    "\n",
    "for j in retain_words:\n",
    "    stops.discard(j)\n",
    "    \n",
    "remove_chars = ['br', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n",
    "               'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '~', '`', '!', '@', '#', '$', '%', '^',\n",
    "               '&', '*', '(', ')', '-', '_', '+', '=', '[', ']', '{', '}', '|', ':', ';', '<', '>', ',',\n",
    "               '.', '?', \",\", '\"']\n",
    "\n",
    "for j in remove_chars:\n",
    "    stops.add(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"Map pos tags to first character lemmatize function accepts\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\" : wordnet.ADJ,\n",
    "               \"N\" : wordnet.NOUN,\n",
    "               \"V\" : wordnet.VERB,\n",
    "               \"R\" : wordnet.ADV\n",
    "               }\n",
    "    \n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# regular expression based tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_tokenizer(text, stops):\n",
    "    \n",
    "    # fix contractions\n",
    "    text2 = contractions.fix(text)\n",
    "    \n",
    "    # tokennzer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words1 = tokenizer.tokenize(text2)\n",
    "    \n",
    "    # remove numbers\n",
    "    #words2 = [x for x in words1 if x.digit() == False]\n",
    "    \n",
    "    # convert to lowercase\n",
    "    words3 = [x.lower() for x in words1]\n",
    "    \n",
    "    # remove stopwords\n",
    "    words4 = [w for w in words3 if w not in stops]\n",
    "    \n",
    "    # use lemmatizer\n",
    "    words5 = [wordnet_lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words4]\n",
    "    \n",
    "    return words5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(df, x_col, y_col=None, max_seq_len=128):\n",
    "    \n",
    "    self.MaxFeature = 20000\n",
    "\n",
    "    if y_col is not None:\n",
    "        data = df[[x_col, y_col]]\n",
    "\n",
    "    else:\n",
    "        data = df[[x_col]]\n",
    "\n",
    "    print(\"Data Reading Completed ...\")\n",
    "    print(\"Train Samples : \", df.shape[0])\n",
    "\n",
    "    # parse and tokenize text data\n",
    "    \n",
    "    data['parse_text'] = data.apply(lambda x : regex_tokenizer(x, stops))\n",
    "    print(\"Tokenization Completed ...\")\n",
    "\n",
    "    # build dictionary\n",
    "    seq_list = data['parse_text'].tolist()\n",
    "    big_list = list(itertools.chain.from_iterable(seq_list))\n",
    "    big_uniq_list = list(set(big_list))\n",
    "\n",
    "    token2idx = {}\n",
    "    for j in range(len(big_uniq_list)):\n",
    "        token2idx[big_uniq_list[j]] = j\n",
    "\n",
    "    # select the top max_features \n",
    "    #???\n",
    "    # add support for padding and unknown tokens\n",
    "    token2idx['<pad>'] = max(token2idx.values())+1\n",
    "    token2idx['<unk>'] = max(token2idx.values())+1\n",
    "\n",
    "    self.token2idx = token2idx\n",
    "    print(\"Dictionary Completed ...\")\n",
    "\n",
    "    # cut long sentences short\n",
    "    data['parse_text_short'] = data['parse_text'].apply(\n",
    "        lambda x : x + [token2idx['<pad>']]*(self.max_seq_len - len(x))\n",
    "        )\n",
    "    print(\"Sentence Normalization Completed ...\")\n",
    "\n",
    "    # convert tokens to indicies\n",
    "    data['tokenized'] = data['parse_text_short'].apply(\n",
    "        lambda x : [token2idx[j] if j in token2idx.keys()\n",
    "                                    else token2idx['<unk>'] for j in x]\n",
    "        )\n",
    "    print(\"Index Conversion Completed ...\")\n",
    "\n",
    "    # add padding to make all samples of equal length\n",
    "    data['tok_pad'] = data['tokenized'].apply(\n",
    "        lambda x : x + [token2idx['<pad>']]*(self.max_seq_len - len(x))\n",
    "        )\n",
    "    print(\"Padding Completed ...\")\n",
    "    \n",
    "    return data, token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design class to read and preprocess data\n",
    "class text_dataset(Dataset):\n",
    "    def __init__(self, df, x_col, y_col=None):\n",
    "    \n",
    "        if y_col is not None:\n",
    "            df = df[[x_col]]\n",
    "            self.target = df[y_col].tolist()\n",
    "            \n",
    "        self.sequence = df['tok_pad'].tolist()\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        if y_col is not None:\n",
    "            return np.array(self.sequence[i]), self.target[i]\n",
    "        else:\n",
    "            self.sequence[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    inputs = torch.LongTensor([item[0] for item in batch])\n",
    "    targets = torch.FloatTensor([item[1] for item in batch])\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, token2idx = text_preprocessing(df, x_col='review', y_col='y', max_seq_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "msk = np.random.rand(len(df)) < train_size\n",
    "train = df[msk]\n",
    "test = df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train dataset\n",
    "t1 = time.time()\n",
    "batch_size = 32\n",
    "max_seq_len = 128\n",
    "train_data = text_dataset(train, x_col = 'tok_pad', y_col = 'y', max_seq_len = max_seq_len)\n",
    "train_data_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True, \n",
    "                              num_workers=4, collate_fn = collate)\n",
    "\n",
    "test_data = text_dataset(test, x_col = 'tok_pad', y_col = 'y', max_seq_len = max_seq_len)\n",
    "test_data_loader = DataLoader(test_data, batch_size = batch_size, shuffle = True, \n",
    "                              num_workers=4, collate_fn = collate)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Time Taken in Text Processing : \", t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embeddings\n",
    "token2idx = train_data.vocab_dictionary()\n",
    "print(\"Length of Dictionary : \", len(token2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(token2idx)\n",
    "embed_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 99\n",
    "np.random.seed(seed)\n",
    "embed_mat = np.random.rand(vocab_size, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_found = 0\n",
    "vocab = token2idx.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = time.time()\n",
    "with open('~/Data/glove.6B/glove.6B.300d.txt', 'rb') as embed_file:\n",
    "    for line in embed_file:\n",
    "        l = line\n",
    "        l = l.decode().split()\n",
    "        word = l[0]\n",
    "        vec = np.array(l[1:]).astype(np.float)\n",
    "        \n",
    "        # check if word is in vocab\n",
    "        if word in vocab:\n",
    "            embed_mat[token2idx['word']] = vec\n",
    "            words_found += 1\n",
    "            \n",
    "print(\"Words found : \", words_found)\n",
    "t4 = time.time()\n",
    "print(\"Time Taken in Embedding Generation : \", t4-t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '~/Data/'\n",
    "filename = path + 'IMDB_Embed'\n",
    "fileObject = open(fileName, 'wb')\n",
    "\n",
    "save = True\n",
    "load = True\n",
    "if save:\n",
    "    pkl.dump(embed_mat, fileObject)\n",
    "    fileObject.close()\n",
    "\n",
    "if load:\n",
    "    fileObject2 = open(fileName, 'wb')\n",
    "    embed_mat = pkl.load(fileObject2)\n",
    "    fileObject2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design GRU model\n",
    "\n",
    "class GRU_Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, embed_mat, non_trainable=True,\n",
    "                gru_layers=2, bidirectional=True):\n",
    "        super(GRU_Model, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embed_mat = embed_mat\n",
    "        \n",
    "        self.gru_layers = gru_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.gru_hidden = 300\n",
    "        self.fc1_size = 200\n",
    "        self.fc2_size = 32\n",
    "        self.output_size =1\n",
    "        \n",
    "        # Define the word embedding layer\n",
    "        self.encoder = nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "        \n",
    "        # Load embedding weights into the layer\n",
    "        embed_weights = torch.tensor(self.embed_mat, dtype=torch.float)\n",
    "        self.encoder.load_state_dict({'weight': embed_weights})\n",
    "        \n",
    "        if non_trainable:\n",
    "            self.encoder.weight.requires_grad = False\n",
    "            \n",
    "        # create a bidirectional GRU layer\n",
    "        self.gru = nn.GRU(self.embed_dim, self.gru_hidden, self.gru_layers, batch_first=True, dropout=0.5, \n",
    "                         bidirectional=self.bidirectional)\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(self.fc1_size)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(self.fc2_size)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "            \n",
    "        self.fc1 = nn.Linear(self.gru_hidden * self.num_directions, self.fc1_size)\n",
    "        self.dropout1 = nn.Dropout(0.10)\n",
    "        self.fc2 = nn.Linear(self.fc1_size, self.fc2_size)\n",
    "        self.dropout2 = nn.Dropout(0.05)\n",
    "        self.fc3 = nn.Linear(self.fc2_size, self.output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        print(\"Input Shape : \", x.shape)\n",
    "        out, hidden = self.gru(self.encoder(x))\n",
    "        print(\"Output Shape : \", out.shape)\n",
    "        out = out[:,-1,:]\n",
    "        out = F.relu(self.batch_norm1(self.fc1(out)))\n",
    "        out = self.dropout1(out)\n",
    "        out = F.relu(self.batch_norm2(self.fc2(out)))\n",
    "        out = self.dropout2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = GRU_Model(vocab_size, embed_dim, embed_mat, non_trainable=True, gru_layers=2, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_epi in range(n_epochs):\n",
    "    print(\"epoch : \", n_epi+1)\n",
    "    step = 0\n",
    "    \n",
    "    t5 = time.time()\n",
    "    \n",
    "    for i,data in enumerate(train_data_loader, 0):\n",
    "        step =step+1\n",
    "        inputs, labels = data\n",
    "        out = model(inputs)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(labels.view(-1,1), out.view(-1,1))\n",
    "        print(\"Step : \", step+1, \" Loss : \", loss.item())\n",
    "        running_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    t6 = time.time()\n",
    "    print(\"Tiem Taken in Training Epoch : \", t6-t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(running_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
